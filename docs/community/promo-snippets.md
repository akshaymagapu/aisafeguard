# Promo Snippets

## X / LinkedIn

Built **AISafe Guard**: open-source LLM safety guardrails for OpenAI + Claude apps.  
Includes prompt injection detection, PII redaction, toxicity checks, and OpenAI-compatible proxy mode.

GitHub: https://github.com/akshaymagapu/aisafeguard  
PyPI: https://pypi.org/project/aisafeguard/

## Reddit (r/LLMOps / r/MachineLearning style)

I open-sourced **AISafe Guard**, a Python toolkit + proxy for adding LLM safety checks with minimal code changes.

Current features:
- prompt injection + jailbreak checks
- PII detection/redaction
- toxicity + malicious URL checks
- OpenAI/Anthropic wrappers
- OpenAI-compatible proxy endpoint

Would appreciate feedback from folks running LLM apps in production:
https://github.com/akshaymagapu/aisafeguard

## Show HN

Show HN: AISafe Guard â€” Open-source safety guardrails for LLM apps (OpenAI + Claude + proxy mode)

GitHub: https://github.com/akshaymagapu/aisafeguard  
PyPI: https://pypi.org/project/aisafeguard/
